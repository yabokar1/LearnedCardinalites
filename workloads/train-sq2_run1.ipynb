{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mscn.util import *\n",
    "from mscn.data import get_train_datasets, load_data, make_dataset\n",
    "from mscn.model import SetConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_torch(vals, min_val, max_val):\n",
    "    vals = (vals * (max_val - min_val)) + min_val\n",
    "    return torch.exp(vals)\n",
    "\n",
    "\n",
    "def qerror_loss(preds, targets, min_val, max_val):\n",
    "    qerror = []\n",
    "    preds = unnormalize_torch(preds, min_val, max_val)\n",
    "    targets = unnormalize_torch(targets, min_val, max_val)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i] / targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i] / preds[i])\n",
    "    return torch.mean(torch.cat(qerror))\n",
    "\n",
    "\n",
    "def predict(model, data_loader, cuda):\n",
    "    preds = []\n",
    "    t_total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "\n",
    "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        if cuda:\n",
    "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
    "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
    "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
    "            targets)\n",
    "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
    "            join_masks)\n",
    "\n",
    "        t = time.time()\n",
    "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "        t_total += time.time() - t\n",
    "\n",
    "        for i in range(outputs.data.shape[0]):\n",
    "            preds.append(outputs.data[i])\n",
    "\n",
    "    return preds, t_total\n",
    "\n",
    "\n",
    "def print_qerror(preds_unnorm, labels_unnorm):\n",
    "    qerror = []\n",
    "    for i in range(len(preds_unnorm)):\n",
    "        # SQ: preds_unnorm[i] is an array whereas labels_unnorm[i] is a scaler (int64)\n",
    "        # It was causing an error, so I changed the following code to unpack the scaler value\n",
    "        # from each array element inside preds_unnorm\n",
    "        if preds_unnorm[i][0] > float(labels_unnorm[i]):\n",
    "            qerror.append(preds_unnorm[i][0] / float(labels_unnorm[i]))\n",
    "        else:\n",
    "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i][0]))\n",
    "\n",
    "    print(\"Median: {}\".format(np.median(qerror)))\n",
    "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
    "    print(\"Max: {}\".format(np.max(qerror)))\n",
    "    print(\"Mean: {}\".format(np.mean(qerror)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args.testset)\n",
    "# train_and_predict(args.testset, args.queries, args.epochs, args.batch, args.hid, args.cuda)\n",
    "#workload_name = 'job-light'\n",
    "workload_name = 'test_tpcds'\n",
    "num_queries = 99\n",
    "num_epochs = 10\n",
    "batch_size = 1024\n",
    "hid_units = 256\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n",
      "Loaded bitmaps\n",
      "min log(label): 0.0\n",
      "max log(label): 19.94772801931604\n",
      "Number of training samples: 89\n",
      "Number of validation samples: 10\n",
      "Created TensorDataset for training data\n",
      "Created TensorDataset for validation data\n"
     ]
    }
   ],
   "source": [
    "# Load training and validation data\n",
    "num_materialized_samples = 1000\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(num_queries, num_materialized_samples)\n",
    "table2vec, column2vec, op2vec, join2vec = dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cast_info ci': array([1., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'movie_companies mc': array([0., 1., 0., 0., 0., 0.], dtype=float32),\n",
       "  'movie_info mi': array([0., 0., 1., 0., 0., 0.], dtype=float32),\n",
       "  'movie_info_idx mi_idx': array([0., 0., 0., 1., 0., 0.], dtype=float32),\n",
       "  'movie_keyword mk': array([0., 0., 0., 0., 1., 0.], dtype=float32),\n",
       "  'title t': array([0., 0., 0., 0., 0., 1.], dtype=float32)},\n",
       " {'ci.person_id': array([1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'ci.role_id': array([0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'mc.company_id': array([0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'mc.company_type_id': array([0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  'mi.info_type_id': array([0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32),\n",
       "  'mi_idx.info_type_id': array([0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32),\n",
       "  'mk.keyword_id': array([0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32),\n",
       "  't.kind_id': array([0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32),\n",
       "  't.production_year': array([0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)},\n",
       " {'<': array([1., 0., 0.], dtype=float32),\n",
       "  '=': array([0., 1., 0.], dtype=float32),\n",
       "  '>': array([0., 0., 1.], dtype=float32)},\n",
       " {'': array([1., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "  't.id=ci.movie_id': array([0., 1., 0., 0., 0., 0.], dtype=float32),\n",
       "  't.id=mc.movie_id': array([0., 0., 1., 0., 0., 0.], dtype=float32),\n",
       "  't.id=mi.movie_id': array([0., 0., 0., 1., 0., 0.], dtype=float32),\n",
       "  't.id=mi_idx.movie_id': array([0., 0., 0., 0., 1., 0.], dtype=float32),\n",
       "  't.id=mk.movie_id': array([0., 0., 0., 0., 0., 1.], dtype=float32)}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 736.8335571289062\n",
      "Epoch 1, loss: 869.8615112304688\n",
      "Epoch 2, loss: 401.494384765625\n",
      "Epoch 3, loss: 402.35174560546875\n",
      "Epoch 4, loss: 361.444091796875\n",
      "Epoch 5, loss: 287.44708251953125\n",
      "Epoch 6, loss: 261.7984619140625\n",
      "Epoch 7, loss: 227.74668884277344\n",
      "Epoch 8, loss: 176.35147094726562\n",
      "Epoch 9, loss: 151.7735137939453\n",
      "Prediction time per training sample: 0.01826982819632198\n",
      "Prediction time per validation sample: 0.0553131103515625\n",
      "\n",
      "Q-Error training set:\n",
      "Median: 23.38071604863853\n",
      "90th percentile: 237.96936131777872\n",
      "95th percentile: 354.34142259421674\n",
      "99th percentile: 1343.1812062252259\n",
      "Max: 4241.5\n",
      "Mean: 130.94419836310914\n",
      "\n",
      "Q-Error validation set:\n",
      "Median: 19.54470109243407\n",
      "90th percentile: 128.16981099862375\n",
      "95th percentile: 492.4071563687199\n",
      "99th percentile: 783.7970326647983\n",
      "Max: 856.6445017388176\n",
      "Mean: 101.817271944403\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '32252.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ghadeer/Downloads/learnedcardinalities/train-sq2_run1.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ghadeer/Downloads/learnedcardinalities/train-sq2_run1.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Load test data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ghadeer/Downloads/learnedcardinalities/train-sq2_run1.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m file_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mworkloads/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m workload_name\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ghadeer/Downloads/learnedcardinalities/train-sq2_run1.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m joins, predicates, tables, samples, label \u001b[39m=\u001b[39m load_data(file_name, num_materialized_samples)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ghadeer/Downloads/learnedcardinalities/train-sq2_run1.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Get feature encoding and proper normalization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ghadeer/Downloads/learnedcardinalities/train-sq2_run1.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m samples_test \u001b[39m=\u001b[39m encode_samples(tables, samples, table2vec)\n",
      "File \u001b[0;32m~/Downloads/learnedcardinalities/mscn/data.py:30\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_name, num_materialized_samples)\u001b[0m\n\u001b[1;32m     28\u001b[0m predicates\u001b[39m.\u001b[39mappend(row[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[39m# SQ: checks the value of cardinality\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mint\u001b[39;49m(row[\u001b[39m3\u001b[39;49m]) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQueries must have non-zero cardinalities\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     exit(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '32252.0'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "sample_feats = len(table2vec) + num_materialized_samples\n",
    "predicate_feats = len(column2vec) + len(op2vec) + 1\n",
    "join_feats = len(join2vec)\n",
    "\n",
    "model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(targets)\n",
    "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(join_masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "        loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
    "\n",
    "# Get final training and validation set predictions\n",
    "preds_train, t_total = predict(model, train_data_loader, cuda)\n",
    "print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
    "\n",
    "preds_test, t_total = predict(model, test_data_loader, cuda)\n",
    "print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "# Unnormalize\n",
    "preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
    "labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
    "\n",
    "preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nQ-Error training set:\")\n",
    "print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
    "\n",
    "print(\"\\nQ-Error validation set:\")\n",
    "print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
    "print(\"\")\n",
    "\n",
    "# Load test data\n",
    "file_name = \"workloads/\" + workload_name\n",
    "joins, predicates, tables, samples, label = load_data(file_name, num_materialized_samples)\n",
    "\n",
    "# Get feature encoding and proper normalization\n",
    "samples_test = encode_samples(tables, samples, table2vec)\n",
    "predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
    "labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
    "\n",
    "print(\"Number of test samples: {}\".format(len(labels_test)))\n",
    "\n",
    "max_num_predicates = max([len(p) for p in predicates_test])\n",
    "max_num_joins = max([len(j) for j in joins_test])\n",
    "\n",
    "# Get test set predictions\n",
    "test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "preds_test, t_total = predict(model, test_data_loader, cuda)\n",
    "print(\"Prediction time per test sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "# Unnormalize\n",
    "preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nQ-Error \" + workload_name + \":\")\n",
    "print_qerror(preds_test_unnorm, label)\n",
    "\n",
    "# Write predictions\n",
    "file_name = \"results/predictions_\" + workload_name + \".csv\"\n",
    "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "with open(file_name, \"w\") as f:\n",
    "    for i in range(len(preds_test_unnorm)):\n",
    "        f.write(str(preds_test_unnorm[i]) + \",\" + label[i] + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

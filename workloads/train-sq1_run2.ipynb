{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'est' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n est ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mscn.util import *\n",
    "from mscn.data import get_train_datasets, load_data, make_dataset\n",
    "from mscn.model import SetConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'est' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n est ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def unnormalize_torch(vals, min_val, max_val):\n",
    "    vals = (vals * (max_val - min_val)) + min_val\n",
    "    return torch.exp(vals)\n",
    "\n",
    "\n",
    "def qerror_loss(preds, targets, min_val, max_val):\n",
    "    qerror = []\n",
    "    preds = unnormalize_torch(preds, min_val, max_val)\n",
    "    targets = unnormalize_torch(targets, min_val, max_val)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i] / targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i] / preds[i])\n",
    "    return torch.mean(torch.cat(qerror))\n",
    "\n",
    "\n",
    "def predict(model, data_loader, cuda):\n",
    "    preds = []\n",
    "    t_total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "\n",
    "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        if cuda:\n",
    "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
    "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
    "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
    "            targets)\n",
    "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
    "            join_masks)\n",
    "\n",
    "        t = time.time()\n",
    "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "        t_total += time.time() - t\n",
    "\n",
    "        for i in range(outputs.data.shape[0]):\n",
    "            preds.append(outputs.data[i])\n",
    "\n",
    "    return preds, t_total\n",
    "\n",
    "\n",
    "def print_qerror(preds_unnorm, labels_unnorm):\n",
    "    qerror = []\n",
    "    for i in range(len(preds_unnorm)):\n",
    "        # SQ: preds_unnorm[i] is an array whereas labels_unnorm[i] is a scaler (int64)\n",
    "        # It was causing an error, so I changed the following code to unpack the scaler value\n",
    "        # from each array element inside preds_unnorm\n",
    "        if preds_unnorm[i][0] > float(labels_unnorm[i]):\n",
    "            qerror.append(preds_unnorm[i][0] / float(labels_unnorm[i]))\n",
    "        else:\n",
    "            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i][0]))\n",
    "\n",
    "    print(\"Median: {}\".format(np.median(qerror)))\n",
    "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
    "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
    "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
    "    print(\"Max: {}\".format(np.max(qerror)))\n",
    "    print(\"Mean: {}\".format(np.mean(qerror)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args.testset)\n",
    "# train_and_predict(args.testset, args.queries, args.epochs, args.batch, args.hid, args.cuda)\n",
    "workload_name = 'job-light'\n",
    "num_queries = 100000\n",
    "num_epochs = 10\n",
    "batch_size = 1024\n",
    "hid_units = 256\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n",
      "Loaded bitmaps\n",
      "min log(label): 0.0\n",
      "max log(label): 19.94772801931604\n",
      "Number of training samples: 90000\n",
      "Number of validation samples: 10000\n",
      "Created TensorDataset for training data\n",
      "Created TensorDataset for validation data\n",
      "Epoch 0, loss: 87.94611702182077\n",
      "Epoch 1, loss: 9.152028847824443\n",
      "Epoch 2, loss: 6.51227083531293\n",
      "Epoch 3, loss: 5.602087746966969\n",
      "Epoch 4, loss: 5.2040769837119365\n",
      "Epoch 5, loss: 4.928066570650447\n",
      "Epoch 6, loss: 4.732810521667654\n",
      "Epoch 7, loss: 4.569513911550695\n",
      "Epoch 8, loss: 4.452799834988334\n",
      "Epoch 9, loss: 4.348687020215121\n",
      "Prediction time per training sample: 0.011015070809258355\n",
      "Prediction time per validation sample: 0.0111663818359375\n",
      "\n",
      "Q-Error training set:\n",
      "Median: 1.837569303617152\n",
      "90th percentile: 6.628471198711917\n",
      "95th percentile: 12.214382239382244\n",
      "99th percentile: 45.0\n",
      "Max: 1408.3290322580644\n",
      "Mean: 4.180855109543047\n",
      "\n",
      "Q-Error validation set:\n",
      "Median: 1.878830593089245\n",
      "90th percentile: 7.400454086129686\n",
      "95th percentile: 14.823587260519819\n",
      "99th percentile: 52.07126180333432\n",
      "Max: 1051.0\n",
      "Mean: 4.879759721625275\n",
      "\n",
      "Loaded queries\n",
      "Loaded bitmaps\n",
      "Number of test samples: 70\n",
      "Prediction time per test sample: 0.026457650320870538\n",
      "\n",
      "Q-Error job-light:\n",
      "Median: 5.779919458124015\n",
      "90th percentile: 94.88421143525929\n",
      "95th percentile: 190.54012234183813\n",
      "99th percentile: 1486.8986721406027\n",
      "Max: 2930.4410460384106\n",
      "Mean: 77.9012671374608\n"
     ]
    }
   ],
   "source": [
    "# Load training and validation data\n",
    "num_materialized_samples = 1000\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(num_queries, num_materialized_samples)\n",
    "table2vec, column2vec, op2vec, join2vec = dicts\n",
    "\n",
    "# Train model\n",
    "sample_feats = len(table2vec) + num_materialized_samples\n",
    "predicate_feats = len(column2vec) + len(op2vec) + 1\n",
    "join_feats = len(join2vec)\n",
    "\n",
    "model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(targets)\n",
    "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(join_masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
    "        loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
    "\n",
    "# Get final training and validation set predictions\n",
    "preds_train, t_total = predict(model, train_data_loader, cuda)\n",
    "print(\"Prediction time per training sample: {}\".format(t_total / len(labels_train) * 1000))\n",
    "\n",
    "preds_test, t_total = predict(model, test_data_loader, cuda)\n",
    "print(\"Prediction time per validation sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "# Unnormalize\n",
    "preds_train_unnorm = unnormalize_labels(preds_train, min_val, max_val)\n",
    "labels_train_unnorm = unnormalize_labels(labels_train, min_val, max_val)\n",
    "\n",
    "preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nQ-Error training set:\")\n",
    "print_qerror(preds_train_unnorm, labels_train_unnorm)\n",
    "\n",
    "print(\"\\nQ-Error validation set:\")\n",
    "print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
    "print(\"\")\n",
    "\n",
    "# Load test data\n",
    "file_name = \"workloads/\" + workload_name\n",
    "joins, predicates, tables, samples, label = load_data(file_name, num_materialized_samples)\n",
    "\n",
    "# Get feature encoding and proper normalization\n",
    "samples_test = encode_samples(tables, samples, table2vec)\n",
    "predicates_test, joins_test = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
    "labels_test, _, _ = normalize_labels(label, min_val, max_val)\n",
    "\n",
    "print(\"Number of test samples: {}\".format(len(labels_test)))\n",
    "\n",
    "max_num_predicates = max([len(p) for p in predicates_test])\n",
    "max_num_joins = max([len(j) for j in joins_test])\n",
    "\n",
    "# Get test set predictions\n",
    "test_data = make_dataset(samples_test, predicates_test, joins_test, labels_test, max_num_joins, max_num_predicates)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "preds_test, t_total = predict(model, test_data_loader, cuda)\n",
    "print(\"Prediction time per test sample: {}\".format(t_total / len(labels_test) * 1000))\n",
    "\n",
    "# Unnormalize\n",
    "preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nQ-Error \" + workload_name + \":\")\n",
    "print_qerror(preds_test_unnorm, label)\n",
    "\n",
    "# Write predictions\n",
    "file_name = \"results/predictions_\" + workload_name + \".csv\"\n",
    "os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "with open(file_name, \"w\") as f:\n",
    "    for i in range(len(preds_test_unnorm)):\n",
    "        f.write(str(preds_test_unnorm[i]) + \",\" + label[i] + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

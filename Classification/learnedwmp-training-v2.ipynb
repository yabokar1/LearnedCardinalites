{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mscn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmscn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmscn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m load_and_encode_train_data, load_data, get_train_datasets\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mscn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "from mscn.util_v2 import *\n",
    "from mscn.data_v2 import load_and_encode_train_data, load_data, get_train_datasets\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from mscn.data import load_data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mscn.data import get_train_datasets, load_data, make_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tpcds'\n",
    "file_name_queries = \"train_{}.csv\".format(dataset_name)\n",
    "\n",
    "joins, predicates, tables, samples, label, num_queries = load_data(file_name_queries)\n",
    "\n",
    "# error: ValueError: too many values to unpack (expected 5)\n",
    "# this error means the function is returning more variables than what we're providing in the output variable list\n",
    "# For example, \n",
    "#       1 dataset_name = 'tpcds'\n",
    "#      2 file_name_queries = \"data/train_{}.csv\".format(dataset_name)\n",
    "#----> 4 joins, predicates, tables, samples, label = load_data(file_name_queries)\n",
    "# ValueError: too many values to unpack (expected 5)\n",
    "# for example, here the load_data is returning 6 items, but I have listed only 5. \n",
    "# so, there is a mismatch between the number of returned items and the expected returned items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(joins))\n",
    "print(len(joins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joins[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicates[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and encode train data by calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n",
      "min log(label): 4.028845579989405\n",
      "max log(label): 6.809432164846193\n",
      "Number of training samples: 12288\n",
      "Number of validation samples: 1366\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tpcds'\n",
    "file_name_queries = \"data/train_{}.csv\".format(dataset_name)\n",
    "\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'store_sales', 'customer_address', 'date_dim', 'call_center', 'store', 'date_dim d3', 'date_dim d1', 'inventory', 'web_returns', 'customer_demographics', 'customer c', 'web_sales', 'date_dim d2', 'customer_demographics cd1', 'customer_demographics cd2', 'customer_address ca', 'catalog_returns cr1', 'itemdate_dim', 'store_returns', 'customer', 'catalog_sales cs1', 'catalog_sales cs2', 'catalog_sales', 'warehouse', 'item', 'household_demographics'}\n"
     ]
    }
   ],
   "source": [
    "# let's print the list of unique tables\n",
    "table_names = get_all_table_names(tables)\n",
    "print(table_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2vec, idx2table = get_set_encoding(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'call_center': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'catalog_returns cr1': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'catalog_sales': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'catalog_sales cs1': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'catalog_sales cs2': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'customer': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'customer c': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'customer_address': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'customer_address ca': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'customer_demographics': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'customer_demographics cd1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'customer_demographics cd2': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'date_dim': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'date_dim d1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'date_dim d2': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'date_dim d3': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'household_demographics': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'inventory': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'item': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'itemdate_dim': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'store': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'store_returns': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32),\n",
       " 'store_sales': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32),\n",
       " 'warehouse': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32),\n",
       " 'web_returns': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32),\n",
       " 'web_sales': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2vec['inventory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tpcds'\n",
    "\n",
    "file_name_queries = \"data/train_{}.csv\".format(dataset_name)\n",
    "\n",
    "# SQ: changed the following code to read the columns min and max for the tpcds dataset\n",
    "file_name_column_min_max_vals = \"data/{}_column_min_max_vals.csv\".format(dataset_name)\n",
    "\n",
    "tables, joins, predicates, label, num_queries = load_data(file_name_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now calling get_train_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n",
      "min log(label): 4.028845579989405\n",
      "max log(label): 6.809432164846193\n",
      "Number of training samples: 12288\n",
      "Number of validation samples: 1366\n",
      "The sample is []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v2.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v2.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtpcds\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v2.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data \u001b[39m=\u001b[39m get_train_datasets(dataset_name)\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/mscn/data_v2.py:175\u001b[0m, in \u001b[0;36mget_train_datasets\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_train_datasets\u001b[39m(dataset_name):\n\u001b[1;32m    174\u001b[0m     dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data \u001b[39m=\u001b[39m load_and_encode_train_data(dataset_name)\n\u001b[0;32m--> 175\u001b[0m     train_dataset \u001b[39m=\u001b[39m make_dataset(\u001b[39m*\u001b[39;49mtrain_data, labels\u001b[39m=\u001b[39;49mlabels_train, max_num_joins\u001b[39m=\u001b[39;49mmax_num_joins,\n\u001b[1;32m    176\u001b[0m                                  max_num_predicates\u001b[39m=\u001b[39;49mmax_num_predicates)\n\u001b[1;32m    177\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreated TensorDataset for training data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     test_dataset \u001b[39m=\u001b[39m make_dataset(\u001b[39m*\u001b[39mtest_data, labels\u001b[39m=\u001b[39mlabels_test, max_num_joins\u001b[39m=\u001b[39mmax_num_joins,\n\u001b[1;32m    179\u001b[0m                                 max_num_predicates\u001b[39m=\u001b[39mmax_num_predicates)\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/mscn/data_v2.py:132\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(samples, predicates, joins, labels, max_num_joins, max_num_predicates)\u001b[0m\n\u001b[1;32m    130\u001b[0m     sample_tensors\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mexpand_dims(sample_tensor, \u001b[39m0\u001b[39m))\n\u001b[1;32m    131\u001b[0m     sample_masks\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mexpand_dims(sample_mask, \u001b[39m0\u001b[39m))\n\u001b[0;32m--> 132\u001b[0m sample_tensors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvstack(sample_tensors)\n\u001b[1;32m    133\u001b[0m sample_tensors \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(sample_tensors)\n\u001b[1;32m    134\u001b[0m sample_masks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(sample_masks)\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tpcds'\n",
    "\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = get_all_table_names(tables)\n",
    "print(table_names)\n",
    "print(len(table_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2vec, idx2table = get_set_encoding(table_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(table2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(idx2table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx2table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables, joins, predicates, label, num_queries = load_data(file_name_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_set = get_all_joins(joins)\n",
    "\n",
    "join_set = [x for x in join_set if x!= '']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(join_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(join_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "join2vec, idx2join = get_set_encoding(join_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_encode_train_data(dataset_name):\n",
    "\n",
    "    # SQ: renamed the train.csv (which had the training dataset for job) to train_job.csv\n",
    "    # SQ: added tpcds train dataset to the data folder and changing the file name below for tpcds\n",
    "    file_name_queries = \"data/train_{}.csv\".format(dataset_name)\n",
    "\n",
    "    # SQ: changed the following code to read the columns min and max for the tpcds dataset\n",
    "    file_name_column_min_max_vals = \"data/{}_column_min_max_vals.csv\".format(dataset_name)\n",
    "\n",
    "    tables, joins, predicates, label, num_queries = load_data(file_name_queries)\n",
    "\n",
    "    # Get table name dict\n",
    "    table_names = get_all_table_names(tables)\n",
    "    table2vec, idx2table = get_set_encoding(table_names)\n",
    "\n",
    "    # Get join name dict\n",
    "    join_set = get_all_joins(joins)\n",
    "    join2vec, idx2join = get_set_encoding(join_set)\n",
    "\n",
    "    # Get column name dict\n",
    "    column_names = get_all_column_names(predicates)\n",
    "    column2vec, idx2column = get_set_encoding(column_names)\n",
    "    \n",
    "    # Get operator name dict\n",
    "    operators = get_all_operators(predicates)\n",
    "    op2vec, idx2op = get_set_encoding(operators)\n",
    "\n",
    "\n",
    "    # Get min and max values for each column\n",
    "    # SQ: changed file open model from rU to r+\n",
    "    with open(file_name_column_min_max_vals, 'r+') as f:\n",
    "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter=','))\n",
    "        column_min_max_vals = {}\n",
    "        for i, row in enumerate(data_raw):\n",
    "            # first row is the header, so skipping it\n",
    "            if i == 0:\n",
    "                continue\n",
    "            # SQ: the following code is checking the data types of the column's min and max values\n",
    "            # If these values are categorical (str), we're hashing them to generate a numeric value\n",
    "            if type(row[1]) is str and type(row[2]) is str:\n",
    "                hash_value_1 = hash(row[1])\n",
    "                hash_value_2 = hash(row[2])\n",
    "                row[1] = (hash_value_1 % 1000)\n",
    "                row[2] = (hash_value_2 % 1000)\n",
    "            column_min_max_vals[row[0]] = [float(row[1]), float(row[2])]\n",
    "\n",
    "    # Get feature encoding and proper normalization\n",
    "    predicates_enc, joins_enc = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
    "    label_norm, min_val, max_val = normalize_labels(label)\n",
    "\n",
    "    # Split in training and validation samples\n",
    "    # SQ: multiply the number of queries by 0.9 and round that number. This gives an index around 0.9 split\n",
    "    num_train = int(num_queries * 0.9)\n",
    "    num_test = num_queries - num_train\n",
    "\n",
    "    predicates_train = predicates_enc[:num_train]\n",
    "    joins_train = joins_enc[:num_train]\n",
    "    labels_train = label_norm[:num_train]\n",
    "\n",
    "    predicates_test = predicates_enc[num_train:num_train + num_test]\n",
    "    joins_test = joins_enc[num_train:num_train + num_test]\n",
    "    labels_test = label_norm[num_train:num_train + num_test]\n",
    "\n",
    "    print(\"Number of training samples: {}\".format(len(labels_train)))\n",
    "    print(\"Number of validation samples: {}\".format(len(labels_test)))\n",
    "\n",
    "    max_num_joins = max(max([len(j) for j in joins_train]), max([len(j) for j in joins_test]))\n",
    "    max_num_predicates = max(max([len(p) for p in predicates_train]), max([len(p) for p in predicates_test]))\n",
    "\n",
    "    dicts = [table2vec, column2vec, op2vec, join2vec]\n",
    "    train_data = [predicates_train, joins_train]\n",
    "    test_data = [predicates_test, joins_test]\n",
    "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data\n",
    "\n",
    "\n",
    "def make_dataset(samples, predicates, joins, labels, max_num_joins, max_num_predicates):\n",
    "    \"\"\"Add zero-padding and wrap as tensor dataset.\"\"\"\n",
    "\n",
    "    sample_masks = []\n",
    "    sample_tensors = []\n",
    "    for sample in samples:\n",
    "        sample_tensor = np.vstack(sample)\n",
    "        num_pad = max_num_joins + 1 - sample_tensor.shape[0]\n",
    "        sample_mask = np.ones_like(sample_tensor).mean(1, keepdims=True)\n",
    "        sample_tensor = np.pad(sample_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        sample_mask = np.pad(sample_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        sample_tensors.append(np.expand_dims(sample_tensor, 0))\n",
    "        sample_masks.append(np.expand_dims(sample_mask, 0))\n",
    "    sample_tensors = np.vstack(sample_tensors)\n",
    "    sample_tensors = torch.FloatTensor(sample_tensors)\n",
    "    sample_masks = np.vstack(sample_masks)\n",
    "    sample_masks = torch.FloatTensor(sample_masks)\n",
    "\n",
    "    predicate_masks = []\n",
    "    predicate_tensors = []\n",
    "    for predicate in predicates:\n",
    "        predicate_tensor = np.vstack(predicate)\n",
    "        num_pad = max_num_predicates - predicate_tensor.shape[0]\n",
    "        predicate_mask = np.ones_like(predicate_tensor).mean(1, keepdims=True)\n",
    "        predicate_tensor = np.pad(predicate_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        predicate_mask = np.pad(predicate_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        predicate_tensors.append(np.expand_dims(predicate_tensor, 0))\n",
    "        predicate_masks.append(np.expand_dims(predicate_mask, 0))\n",
    "    predicate_tensors = np.vstack(predicate_tensors)\n",
    "    predicate_tensors = torch.FloatTensor(predicate_tensors)\n",
    "    predicate_masks = np.vstack(predicate_masks)\n",
    "    predicate_masks = torch.FloatTensor(predicate_masks)\n",
    "\n",
    "    join_masks = []\n",
    "    join_tensors = []\n",
    "    for join in joins:\n",
    "        join_tensor = np.vstack(join)\n",
    "        num_pad = max_num_joins - join_tensor.shape[0]\n",
    "        join_mask = np.ones_like(join_tensor).mean(1, keepdims=True)\n",
    "        join_tensor = np.pad(join_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        join_mask = np.pad(join_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        join_tensors.append(np.expand_dims(join_tensor, 0))\n",
    "        join_masks.append(np.expand_dims(join_mask, 0))\n",
    "    join_tensors = np.vstack(join_tensors)\n",
    "    join_tensors = torch.FloatTensor(join_tensors)\n",
    "    join_masks = np.vstack(join_masks)\n",
    "    join_masks = torch.FloatTensor(join_masks)\n",
    "\n",
    "    target_tensor = torch.FloatTensor(labels)\n",
    "\n",
    "    return dataset.TensorDataset(predicate_tensors, join_tensors, target_tensor, predicate_masks, join_masks)\n",
    "\n",
    "\n",
    "class SetConv(nn.Module):\n",
    "    def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
    "        super(SetConv, self).__init__()\n",
    "        self.sample_mlp1 = nn.Linear(sample_feats, hid_units)\n",
    "        self.sample_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.predicate_mlp1 = nn.Linear(predicate_feats, hid_units)\n",
    "        self.predicate_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.join_mlp1 = nn.Linear(join_feats, hid_units)\n",
    "        self.join_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.out_mlp1 = nn.Linear(hid_units * 3, hid_units)\n",
    "        self.out_mlp2 = nn.Linear(hid_units, 1)\n",
    "\n",
    "    def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
    "        # samples has shape [batch_size x num_joins+1 x sample_feats]\n",
    "        # predicates has shape [batch_size x num_predicates x predicate_feats]\n",
    "        # joins has shape [batch_size x num_joins x join_feats]\n",
    "\n",
    "        hid_sample = F.relu(self.sample_mlp1(samples))\n",
    "        hid_sample = F.relu(self.sample_mlp2(hid_sample))\n",
    "        hid_sample = hid_sample * sample_mask  # Mask\n",
    "        hid_sample = torch.sum(hid_sample, dim=1, keepdim=False)\n",
    "        sample_norm = sample_mask.sum(1, keepdim=False)\n",
    "        hid_sample = hid_sample / sample_norm  # Calculate average only over non-masked parts\n",
    "\n",
    "        hid_predicate = F.relu(self.predicate_mlp1(predicates))\n",
    "        hid_predicate = F.relu(self.predicate_mlp2(hid_predicate))\n",
    "        hid_predicate = hid_predicate * predicate_mask\n",
    "        hid_predicate = torch.sum(hid_predicate, dim=1, keepdim=False)\n",
    "        predicate_norm = predicate_mask.sum(1, keepdim=False)\n",
    "        hid_predicate = hid_predicate / predicate_norm\n",
    "\n",
    "        hid_join = F.relu(self.join_mlp1(joins))\n",
    "        hid_join = F.relu(self.join_mlp2(hid_join))\n",
    "        hid_join = hid_join * join_mask\n",
    "        hid_join = torch.sum(hid_join, dim=1, keepdim=False)\n",
    "        join_norm = join_mask.sum(1, keepdim=False)\n",
    "        hid_join = hid_join / join_norm\n",
    "\n",
    "        hid = torch.cat((hid_sample, hid_predicate, hid_join), 1)\n",
    "        hid = F.relu(self.out_mlp1(hid))\n",
    "        out = torch.sigmoid(self.out_mlp2(hid))\n",
    "        return out\n",
    "\n",
    "def unnormalize_torch(vals, min_val, max_val):\n",
    "    vals = (vals * (max_val - min_val)) + min_val\n",
    "    return torch.exp(vals)\n",
    "\n",
    "def qerror_loss(preds, targets, min_val, max_val):\n",
    "    qerror = []\n",
    "    preds = unnormalize_torch(preds, min_val, max_val)\n",
    "    targets = unnormalize_torch(targets, min_val, max_val)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i] / targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i] / preds[i])\n",
    "    return torch.mean(torch.cat(qerror))\n",
    "\n",
    "def get_train_datasets(dataset_name):\n",
    "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(dataset_name)\n",
    "    train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins,\n",
    "                                 max_num_predicates=max_num_predicates)\n",
    "    print(\"Created TensorDataset for training data\")\n",
    "    test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins,\n",
    "                                max_num_predicates=max_num_predicates)\n",
    "    print(\"Created TensorDataset for validation data\")\n",
    "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing load_and_encode_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tpcds'\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing get_train_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tpcds'\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET_TRAIN_DATA steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tpcds'\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(dataset_name)\n",
    "\n",
    "table2vec, column2vec, op2vec, join2vec = dicts\n",
    "\n",
    "train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins, max_num_predicates=max_num_predicates)\n",
    "\n",
    "test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins, max_num_predicates=max_num_predicates)\n",
    "print(\"Created TensorDataset for validation data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to explore a TensorDataset\n",
    "# Generated by WCA for GP\n",
    "# Print the length of the dataset\n",
    "print(len(train_dataset))\n",
    "\n",
    "# Print the first element of the dataset\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "hid_units = 256\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "sample_feats = len(table2vec)\n",
    "predicate_feats = len(column2vec) + len(op2vec) + 1\n",
    "join_feats = len(join2vec)\n",
    "\n",
    "model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "        predicates, joins, targets, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        predicates, joins, targets = Variable(predicates), Variable(joins), Variable(targets)\n",
    "        \n",
    "        predicate_masks, join_masks = Variable(predicate_masks), Variable(join_masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(predicates, joins, predicate_masks, join_masks)\n",
    "        loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# troubleshooting file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'tpcds'\n",
    "file_name = \"data/train_{}.csv\".format(dataset_name)\n",
    "\n",
    "joins = []\n",
    "predicates = []\n",
    "tables = []\n",
    "label = []\n",
    "\n",
    "num_queries = 0\n",
    "\n",
    "    # Load queries\n",
    "    # SQ: changed file open mode from 'rU' to 'r+'\n",
    "    # SQ: the following function loads training data from train.csv file. \n",
    "    # train.csv file has both the input features and the actual cardinality of training queries\n",
    "    # the following code block reads 1 training sample at a time, tokenize each training sample by # \n",
    "    # here '#' separates different kinds of information packed into each training sample: \n",
    "    # These are TABLES, JOINS, PREDICATES, and CARDINALITY. Since this generates a list of tokens and the \n",
    "    # list index is 0-based, the position 3 in the list is the cardinality. \n",
    "    # tables#joins#predicates#db2#actual#template\n",
    "\n",
    "df = pd.read_csv(file_name, delimiter='#')\n",
    "\n",
    "df = df[df['actual'] >0]\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "num_queries = df.shape[0]\n",
    "\n",
    "    # how to iterate rows in a dataframe?\n",
    "    # Generated by WCA for GP\n",
    "for index, row in df.iterrows():\n",
    "    #tables.append(row['tables'].split(','))\n",
    "    # joins.append(row['joins'].split(','))\n",
    "    print(str(row['predicates']).split(','))\n",
    "    #predicates.append(row['predicates'].split(','))\n",
    "    #label.append(row['actual'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a466aaca05f99992355058fd211f5f1ccaafc4256e27d44cd1d3121623e9a21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "from mscn.util import *\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from mscn.data import load_data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mscn.data import get_train_datasets, load_data, make_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    joins = []\n",
    "    predicates = []\n",
    "    tables = []\n",
    "    label = []\n",
    "\n",
    "    num_queries = 0\n",
    "\n",
    "    # Load queries\n",
    "    # SQ: changed file open mode from 'rU' to 'r+'\n",
    "    # SQ: the following function loads training data from train.csv file. \n",
    "    # train.csv file has both the input features and the actual cardinality of training queries\n",
    "    # the following code block reads 1 training sample at a time, tokenize each training sample by # \n",
    "    # here '#' separates different kinds of information packed into each training sample: \n",
    "    # These are TABLES, JOINS, PREDICATES, and CARDINALITY. Since this generates a list of tokens and the \n",
    "    # list index is 0-based, the position 3 in the list is the cardinality. \n",
    "    # tables#joins#predicates#db2#actual#template\n",
    "    with open(file_name, 'r+') as f:\n",
    "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter='#'))\n",
    "\n",
    "        num_queries = len(data_raw)\n",
    "\n",
    "        for i, row in enumerate(data_raw):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            tables.append(row[0].split(','))\n",
    "            joins.append(row[1].split(','))\n",
    "            predicates.append(row[2].split(','))\n",
    "            # SQ: checks the value of cardinality\n",
    "            # SQ: changed the row index to 4 as in our tpcds dataset the value of the label is in the column index 4\n",
    "            # if int(row[3]) < 1:\n",
    "            # SQ: changed the type checking below from int to float\n",
    "            if float(row[4]) < 1.0:\n",
    "                print(\"Queries must have non-zero cardinalities\")\n",
    "                exit(1)\n",
    "            # SQ: changed the row index to 4, which has the value of the label in our tpcds dataset\n",
    "            # label.append(row[3])\n",
    "            label.append(row[4])\n",
    "    print(\"Loaded queries\")\n",
    "\n",
    "    # Split predicates\n",
    "    predicates = [list(chunks(d, 3)) for d in predicates]\n",
    "\n",
    "    return tables, joins, predicates, label, num_queries\n",
    "\n",
    "def load_and_encode_train_data(dataset_name):\n",
    "\n",
    "    # SQ: renamed the train.csv (which had the training dataset for job) to train_job.csv\n",
    "    # SQ: added tpcds train dataset to the data folder and changing the file name below for tpcds\n",
    "    file_name_queries = \"data/train_{}.csv\".format(dataset_name)\n",
    "\n",
    "    # SQ: changed the following code to read the columns min and max for the tpcds dataset\n",
    "    file_name_column_min_max_vals = \"data/{}_column_min_max_vals.csv\".format(dataset_name)\n",
    "\n",
    "    tables, joins, predicates, label, num_queries = load_data(file_name_queries)\n",
    "\n",
    "    # Get table name dict\n",
    "    table_names = get_all_table_names(tables)\n",
    "    table2vec, idx2table = get_set_encoding(table_names)\n",
    "\n",
    "    # Get join name dict\n",
    "    join_set = get_all_joins(joins)\n",
    "    join2vec, idx2join = get_set_encoding(join_set)\n",
    "\n",
    "    # Get column name dict\n",
    "    column_names = get_all_column_names(predicates)\n",
    "    column2vec, idx2column = get_set_encoding(column_names)\n",
    "    \n",
    "    # Get operator name dict\n",
    "    operators = get_all_operators(predicates)\n",
    "    op2vec, idx2op = get_set_encoding(operators)\n",
    "\n",
    "\n",
    "    # Get min and max values for each column\n",
    "    # SQ: changed file open model from rU to r+\n",
    "    with open(file_name_column_min_max_vals, 'r+') as f:\n",
    "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter=','))\n",
    "        column_min_max_vals = {}\n",
    "        for i, row in enumerate(data_raw):\n",
    "            # first row is the header, so skipping it\n",
    "            if i == 0:\n",
    "                continue\n",
    "            # SQ: the following code is checking the data types of the column's min and max values\n",
    "            # If these values are categorical (str), we're hashing them to generate a numeric value\n",
    "            if type(row[1]) is str and type(row[2]) is str:\n",
    "                hash_value_1 = hash(row[1])\n",
    "                hash_value_2 = hash(row[2])\n",
    "                row[1] = (hash_value_1 % 1000)\n",
    "                row[2] = (hash_value_2 % 1000)\n",
    "            column_min_max_vals[row[0]] = [float(row[1]), float(row[2])]\n",
    "\n",
    "    # Get feature encoding and proper normalization\n",
    "    predicates_enc, joins_enc = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
    "    label_norm, min_val, max_val = normalize_labels(label)\n",
    "\n",
    "    # Split in training and validation samples\n",
    "    # SQ: multiply the number of queries by 0.9 and round that number. This gives an index around 0.9 split\n",
    "    num_train = int(num_queries * 0.9)\n",
    "    num_test = num_queries - num_train\n",
    "\n",
    "    predicates_train = predicates_enc[:num_train]\n",
    "    joins_train = joins_enc[:num_train]\n",
    "    labels_train = label_norm[:num_train]\n",
    "\n",
    "    predicates_test = predicates_enc[num_train:num_train + num_test]\n",
    "    joins_test = joins_enc[num_train:num_train + num_test]\n",
    "    labels_test = label_norm[num_train:num_train + num_test]\n",
    "\n",
    "    print(\"Number of training samples: {}\".format(len(labels_train)))\n",
    "    print(\"Number of validation samples: {}\".format(len(labels_test)))\n",
    "\n",
    "    max_num_joins = max(max([len(j) for j in joins_train]), max([len(j) for j in joins_test]))\n",
    "    max_num_predicates = max(max([len(p) for p in predicates_train]), max([len(p) for p in predicates_test]))\n",
    "\n",
    "    dicts = [table2vec, column2vec, op2vec, join2vec]\n",
    "    train_data = [predicates_train, joins_train]\n",
    "    test_data = [predicates_test, joins_test]\n",
    "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data\n",
    "\n",
    "\n",
    "def make_dataset(predicates, joins, labels, max_num_joins, max_num_predicates):\n",
    "    \"\"\"Add zero-padding and wrap as tensor dataset.\"\"\"\n",
    "\n",
    "    predicate_masks = []\n",
    "    predicate_tensors = []\n",
    "    for predicate in predicates:\n",
    "        predicate_tensor = np.vstack(predicate)\n",
    "        num_pad = max_num_predicates - predicate_tensor.shape[0]\n",
    "        predicate_mask = np.ones_like(predicate_tensor).mean(1, keepdims=True)\n",
    "        predicate_tensor = np.pad(predicate_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        predicate_mask = np.pad(predicate_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        predicate_tensors.append(np.expand_dims(predicate_tensor, 0))\n",
    "        predicate_masks.append(np.expand_dims(predicate_mask, 0))\n",
    "    predicate_tensors = np.vstack(predicate_tensors)\n",
    "    predicate_tensors = torch.FloatTensor(predicate_tensors)\n",
    "    predicate_masks = np.vstack(predicate_masks)\n",
    "    predicate_masks = torch.FloatTensor(predicate_masks)\n",
    "\n",
    "    join_masks = []\n",
    "    join_tensors = []\n",
    "    for join in joins:\n",
    "        join_tensor = np.vstack(join)\n",
    "        num_pad = max_num_joins - join_tensor.shape[0]\n",
    "        join_mask = np.ones_like(join_tensor).mean(1, keepdims=True)\n",
    "        join_tensor = np.pad(join_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
    "        join_mask = np.pad(join_mask, ((0, num_pad), (0, 0)), 'constant')\n",
    "        join_tensors.append(np.expand_dims(join_tensor, 0))\n",
    "        join_masks.append(np.expand_dims(join_mask, 0))\n",
    "    join_tensors = np.vstack(join_tensors)\n",
    "    join_tensors = torch.FloatTensor(join_tensors)\n",
    "    join_masks = np.vstack(join_masks)\n",
    "    join_masks = torch.FloatTensor(join_masks)\n",
    "\n",
    "    target_tensor = torch.FloatTensor(labels)\n",
    "\n",
    "    return dataset.TensorDataset(predicate_tensors, join_tensors, target_tensor, predicate_masks, join_masks)\n",
    "\n",
    "\n",
    "class SetConv(nn.Module):\n",
    "    def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
    "        super(SetConv, self).__init__()\n",
    "        self.sample_mlp1 = nn.Linear(sample_feats, hid_units)\n",
    "        self.sample_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.predicate_mlp1 = nn.Linear(predicate_feats, hid_units)\n",
    "        self.predicate_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.join_mlp1 = nn.Linear(join_feats, hid_units)\n",
    "        self.join_mlp2 = nn.Linear(hid_units, hid_units)\n",
    "        self.out_mlp1 = nn.Linear(hid_units * 3, hid_units)\n",
    "        self.out_mlp2 = nn.Linear(hid_units, 1)\n",
    "\n",
    "    def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
    "        # samples has shape [batch_size x num_joins+1 x sample_feats]\n",
    "        # predicates has shape [batch_size x num_predicates x predicate_feats]\n",
    "        # joins has shape [batch_size x num_joins x join_feats]\n",
    "\n",
    "        hid_sample = F.relu(self.sample_mlp1(samples))\n",
    "        hid_sample = F.relu(self.sample_mlp2(hid_sample))\n",
    "        hid_sample = hid_sample * sample_mask  # Mask\n",
    "        hid_sample = torch.sum(hid_sample, dim=1, keepdim=False)\n",
    "        sample_norm = sample_mask.sum(1, keepdim=False)\n",
    "        hid_sample = hid_sample / sample_norm  # Calculate average only over non-masked parts\n",
    "\n",
    "        hid_predicate = F.relu(self.predicate_mlp1(predicates))\n",
    "        hid_predicate = F.relu(self.predicate_mlp2(hid_predicate))\n",
    "        hid_predicate = hid_predicate * predicate_mask\n",
    "        hid_predicate = torch.sum(hid_predicate, dim=1, keepdim=False)\n",
    "        predicate_norm = predicate_mask.sum(1, keepdim=False)\n",
    "        hid_predicate = hid_predicate / predicate_norm\n",
    "\n",
    "        hid_join = F.relu(self.join_mlp1(joins))\n",
    "        hid_join = F.relu(self.join_mlp2(hid_join))\n",
    "        hid_join = hid_join * join_mask\n",
    "        hid_join = torch.sum(hid_join, dim=1, keepdim=False)\n",
    "        join_norm = join_mask.sum(1, keepdim=False)\n",
    "        hid_join = hid_join / join_norm\n",
    "\n",
    "        hid = torch.cat((hid_sample, hid_predicate, hid_join), 1)\n",
    "        hid = F.relu(self.out_mlp1(hid))\n",
    "        out = torch.sigmoid(self.out_mlp2(hid))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET_TRAIN_DATA steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded queries\n",
      "min log(label): 4.028845579989405\n",
      "max log(label): 6.809432164846193\n",
      "Number of training samples: 12289\n",
      "Number of validation samples: 1365\n",
      "Created TensorDataset for validation data\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tpcds'\n",
    "dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(dataset_name)\n",
    "\n",
    "table2vec, column2vec, op2vec, join2vec = dicts\n",
    "\n",
    "train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins, max_num_predicates=max_num_predicates)\n",
    "\n",
    "test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins, max_num_predicates=max_num_predicates)\n",
    "print(\"Created TensorDataset for validation data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.TensorDataset"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12289\n",
      "(tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7827],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000]]), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]]), tensor(0.5591), tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]]))\n"
     ]
    }
   ],
   "source": [
    "# write code to explore a TensorDataset\n",
    "# Generated by WCA for GP\n",
    "# Print the length of the dataset\n",
    "print(len(train_dataset))\n",
    "\n",
    "# Print the first element of the dataset\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 1024\n",
    "hid_units = 256\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "sample_feats = len(table2vec)\n",
    "predicate_feats = len(column2vec) + len(op2vec) + 1\n",
    "join_feats = len(join2vec)\n",
    "\n",
    "model = SetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v1.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v1.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v1.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss_total \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v1.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m batch_idx, data_batch \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(train_data_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v1.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         predicates, joins, targets, predicate_masks, join_masks \u001b[39m=\u001b[39;49m data_batch\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaikhq/coding/learnedcardinalities/learnedwmp-training-v1.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         predicates, joins, targets \u001b[39m=\u001b[39;49m Variable(predicates), Variable(joins), Variable(targets)\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:145\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type([collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed])\n\u001b[1;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    148\u001b[0m         \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:145\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type([collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed])\n\u001b[1;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    148\u001b[0m         \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n",
      "File \u001b[0;32m~/coding/learnedcardinalities/venv/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    136\u001b[0m elem_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mnext\u001b[39m(it))\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(elem) \u001b[39m==\u001b[39m elem_size \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m it):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0.\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(train_data_loader):\n",
    "        predicates, joins, targets, predicate_masks, join_masks = data_batch\n",
    "\n",
    "        predicates, joins, targets = Variable(predicates), Variable(joins), Variable(targets)\n",
    "        \n",
    "        predicate_masks, join_masks = Variable(predicate_masks), Variable(join_masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(predicates, joins, predicate_masks, join_masks)\n",
    "        loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (v3.11.3:f3909b8bc8, Apr  4 2023, 20:12:10) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a466aaca05f99992355058fd211f5f1ccaafc4256e27d44cd1d3121623e9a21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
